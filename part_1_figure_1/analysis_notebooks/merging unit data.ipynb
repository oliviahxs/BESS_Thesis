{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63137c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 raw BESS unit files:\n",
      "  - Centrica - Roosecote.csv\n",
      "  - Conrad - Blackpool.csv\n",
      "  - Conrad - Midsomer.csv\n",
      "  - Conrad - Torquay.csv\n",
      "  - Conrad - Winchester.csv\n",
      "  - EDF - Bustleholm.csv\n",
      "  - EDF - Coventry.csv\n",
      "  - EDF - Cowley.csv\n",
      "  - EDF - Kemsley.csv\n",
      "  - EDF - Tye Lane.csv\n",
      "  - GS - Breach Farm.csv\n",
      "  - GS - Cenin.csv\n",
      "  - GS - Hulley Road.csv\n",
      "  - GS - Larport.csv\n",
      "  - GS - Lascar.csv\n",
      "  - GS - Port of Tilbury.csv\n",
      "  - Scottish - Whitelee.csv\n",
      "\n",
      "Processing: Conrad - Midsomer.csv\n",
      "  ✓ Processed 376 records\n",
      "\n",
      "Processing: GS - Lascar.csv\n",
      "  ✓ Processed 1,835 records\n",
      "\n",
      "Processing: Scottish - Whitelee.csv\n",
      "  ✓ Processed 5,506 records\n",
      "\n",
      "Processing: Conrad - Winchester.csv\n",
      "  ✓ Processed 1,136 records\n",
      "\n",
      "Processing: GS - Port of Tilbury.csv\n",
      "  ✓ Processed 2,624 records\n",
      "\n",
      "Processing: GS - Breach Farm.csv\n",
      "  ✓ Processed 5,506 records\n",
      "\n",
      "Processing: Conrad - Winchester.csv\n",
      "  ✓ Processed 1,136 records\n",
      "\n",
      "Processing: GS - Port of Tilbury.csv\n",
      "  ✓ Processed 2,624 records\n",
      "\n",
      "Processing: GS - Breach Farm.csv\n",
      "  ✓ Processed 2,192 records\n",
      "\n",
      "Processing: EDF - Kemsley.csv\n",
      "  ✓ Processed 2,192 records\n",
      "\n",
      "Processing: EDF - Kemsley.csv\n",
      "  ✓ Processed 7,308 records\n",
      "\n",
      "Processing: EDF - Tye Lane.csv\n",
      "  ✓ Processed 1,717 records\n",
      "\n",
      "Processing: GS - Larport.csv\n",
      "  ✓ Processed 1,947 records\n",
      "\n",
      "Processing: Conrad - Blackpool.csv\n",
      "  ✓ Processed 7,308 records\n",
      "\n",
      "Processing: EDF - Tye Lane.csv\n",
      "  ✓ Processed 1,717 records\n",
      "\n",
      "Processing: GS - Larport.csv\n",
      "  ✓ Processed 1,947 records\n",
      "\n",
      "Processing: Conrad - Blackpool.csv\n",
      "  ✓ Processed 1,507 records\n",
      "\n",
      "Processing: Centrica - Roosecote.csv\n",
      "  ✓ Processed 1,507 records\n",
      "\n",
      "Processing: Centrica - Roosecote.csv\n",
      "  ✓ Processed 6,528 records\n",
      "\n",
      "Processing: EDF - Cowley.csv\n",
      "  ✓ Processed 3,718 records\n",
      "\n",
      "Processing: GS - Hulley Road.csv\n",
      "  ✓ Processed 6,528 records\n",
      "\n",
      "Processing: EDF - Cowley.csv\n",
      "  ✓ Processed 3,718 records\n",
      "\n",
      "Processing: GS - Hulley Road.csv\n",
      "  ✓ Processed 1,578 records\n",
      "\n",
      "Processing: GS - Cenin.csv\n",
      "  ✓ Processed 540 records\n",
      "\n",
      "Processing: EDF - Coventry.csv\n",
      "  ✓ Processed 1,578 records\n",
      "\n",
      "Processing: GS - Cenin.csv\n",
      "  ✓ Processed 540 records\n",
      "\n",
      "Processing: EDF - Coventry.csv\n",
      "  ✓ Processed 6,286 records\n",
      "\n",
      "Processing: EDF - Bustleholm.csv\n",
      "  ✓ Processed 6,286 records\n",
      "\n",
      "Processing: EDF - Bustleholm.csv\n",
      "  ✓ Processed 7,368 records\n",
      "\n",
      "Processing: Conrad - Torquay.csv\n",
      "  ✓ Processed 1,298 records\n",
      "  ✓ Processed 7,368 records\n",
      "\n",
      "Processing: Conrad - Torquay.csv\n",
      "  ✓ Processed 1,298 records\n",
      "\n",
      "==================================================\n",
      "✓ Merged dataset saved: merged_17_units_updated.csv\n",
      "✓ Total records: 53,464\n",
      "✓ Total units: 17\n",
      "✓ Total companies: 5\n",
      "✓ Date range: 2023-11-02 23:00:00 to 2025-06-16 21:30:00\n",
      "✓ Columns: ['registeredAuctionParticipant', 'auctionUnit', 'serviceType', 'auctionProduct', 'Delivery capacity (MW)', 'clearingPrice', 'deliveryStart', 'deliveryEnd', 'Delivery date', 'EFA block of the day', 'source_file', 'Delivery capacity(MW)', 'Delivery quantity (MW)', 'Installed capacity (MW)', 'Installed Capacity (MW)', 'delivery capacity (MW)']\n",
      "\n",
      "Units included:\n",
      "  - Centrica - Roosecote: 6,528 records\n",
      "  - Conrad - Blackpool: 1,507 records\n",
      "  - Conrad - Midsomer: 376 records\n",
      "  - Conrad - Torquay: 1,298 records\n",
      "  - Conrad - Winchester: 1,136 records\n",
      "  - EDF - Bustleholm: 7,368 records\n",
      "  - EDF - Coventry: 6,286 records\n",
      "  - EDF - Cowley: 3,718 records\n",
      "  - EDF - Kemsley: 7,308 records\n",
      "  - EDF - Tye Lane: 1,717 records\n",
      "  - GS - Breach Farm: 2,192 records\n",
      "  - GS - Cenin: 540 records\n",
      "  - GS - Hulley Road: 1,578 records\n",
      "  - GS - Larport: 1,947 records\n",
      "  - GS - Lascar: 1,835 records\n",
      "  - GS - Port of Tilbury: 2,624 records\n",
      "  - Scottish - Whitelee: 5,506 records\n",
      "\n",
      "==================================================\n",
      "✓ Merged dataset saved: merged_17_units_updated.csv\n",
      "✓ Total records: 53,464\n",
      "✓ Total units: 17\n",
      "✓ Total companies: 5\n",
      "✓ Date range: 2023-11-02 23:00:00 to 2025-06-16 21:30:00\n",
      "✓ Columns: ['registeredAuctionParticipant', 'auctionUnit', 'serviceType', 'auctionProduct', 'Delivery capacity (MW)', 'clearingPrice', 'deliveryStart', 'deliveryEnd', 'Delivery date', 'EFA block of the day', 'source_file', 'Delivery capacity(MW)', 'Delivery quantity (MW)', 'Installed capacity (MW)', 'Installed Capacity (MW)', 'delivery capacity (MW)']\n",
      "\n",
      "Units included:\n",
      "  - Centrica - Roosecote: 6,528 records\n",
      "  - Conrad - Blackpool: 1,507 records\n",
      "  - Conrad - Midsomer: 376 records\n",
      "  - Conrad - Torquay: 1,298 records\n",
      "  - Conrad - Winchester: 1,136 records\n",
      "  - EDF - Bustleholm: 7,368 records\n",
      "  - EDF - Coventry: 6,286 records\n",
      "  - EDF - Cowley: 3,718 records\n",
      "  - EDF - Kemsley: 7,308 records\n",
      "  - EDF - Tye Lane: 1,717 records\n",
      "  - GS - Breach Farm: 2,192 records\n",
      "  - GS - Cenin: 540 records\n",
      "  - GS - Hulley Road: 1,578 records\n",
      "  - GS - Larport: 1,947 records\n",
      "  - GS - Lascar: 1,835 records\n",
      "  - GS - Port of Tilbury: 2,624 records\n",
      "  - Scottish - Whitelee: 5,506 records\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def process_raw_bess_data(df):\n",
    "    \"\"\"\n",
    "    Process raw BESS data to add EFA blocks and delivery dates\n",
    "    This replicates the processing logic from individual notebooks\n",
    "    \"\"\"\n",
    "    # Convert deliveryStart to datetime\n",
    "    df['deliveryStart'] = pd.to_datetime(df['deliveryStart'])\n",
    "    \n",
    "    # Clean up column names\n",
    "    df.columns = df.columns.str.strip()\n",
    "    if 'EFA block of the day ' in df.columns:\n",
    "        df = df.rename(columns={'EFA block of the day ': 'EFA block of the day'})\n",
    "    \n",
    "    # Rename executedQuantity to be more descriptive\n",
    "    if 'executedQuantity' in df.columns:\n",
    "        df = df.rename(columns={'executedQuantity': 'Delivery capacity (MW)'})\n",
    "    \n",
    "    # Fill missing delivery dates and EFA blocks\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row.get('Delivery date')) or pd.isna(row.get('EFA block of the day')):\n",
    "            start_time = row['deliveryStart']\n",
    "            hour = start_time.hour\n",
    "            \n",
    "            # EFA blocks mapping:\n",
    "            # Block 1: 23:00-03:00 (delivery date is next day for 23:xx)\n",
    "            # Block 2: 03:00-07:00 \n",
    "            # Block 3: 07:00-11:00 \n",
    "            # Block 4: 11:00-15:00 \n",
    "            # Block 5: 15:00-19:00 \n",
    "            # Block 6: 19:00-23:00 \n",
    "            \n",
    "            if hour >= 23:  # 23:00-23:59\n",
    "                efa_block = 1\n",
    "                delivery_date = (start_time + pd.Timedelta(days=1)).strftime('%d/%m/%Y')\n",
    "            elif hour < 3:  # 00:00-02:59\n",
    "                efa_block = 1\n",
    "                delivery_date = start_time.strftime('%d/%m/%Y')\n",
    "            elif hour < 7:  # 03:00-06:59\n",
    "                efa_block = 2\n",
    "                delivery_date = start_time.strftime('%d/%m/%Y')\n",
    "            elif hour < 11:  # 07:00-10:59\n",
    "                efa_block = 3\n",
    "                delivery_date = start_time.strftime('%d/%m/%Y')\n",
    "            elif hour < 15:  # 11:00-14:59\n",
    "                efa_block = 4\n",
    "                delivery_date = start_time.strftime('%d/%m/%Y')\n",
    "            elif hour < 19:  # 15:00-18:59\n",
    "                efa_block = 5\n",
    "                delivery_date = start_time.strftime('%d/%m/%Y')\n",
    "            else:  # 19:00-22:59\n",
    "                efa_block = 6\n",
    "                delivery_date = start_time.strftime('%d/%m/%Y')\n",
    "            \n",
    "            # Update missing values\n",
    "            if pd.isna(row.get('Delivery date')):\n",
    "                # Convert delivery date column to object type first to avoid pandas warning\n",
    "                if df['Delivery date'].dtype != 'object':\n",
    "                    df['Delivery date'] = df['Delivery date'].astype('object')\n",
    "                df.at[index, 'Delivery date'] = delivery_date\n",
    "            if pd.isna(row.get('EFA block of the day')):\n",
    "                df.at[index, 'EFA block of the day'] = efa_block\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Store the original directory and navigate to raw_data\n",
    "original_dir = os.getcwd()\n",
    "os.chdir('../raw_data')\n",
    "\n",
    "csv_files = [f for f in glob.glob(\"*.csv\") if not f.endswith('_updated.csv') and f != 'Battery name mapping.csv']\n",
    "\n",
    "print(f\"Found {len(csv_files)} raw BESS unit files:\")\n",
    "for file in sorted(csv_files):\n",
    "    print(f\"  - {file}\")\n",
    "\n",
    "# Load the battery mapping to check which units have actual data files\n",
    "mapping_df = pd.read_csv('Battery name mapping.csv')\n",
    "print(f\"\\nOriginal mapping file has {len(mapping_df)} units\")\n",
    "\n",
    "# Create a list of units that have actual data files\n",
    "# This will exclude Conrad Energy - Swindon (SWDN-1) and Ecotricity - Alveston (ALVB-1)\n",
    "# since they don't have corresponding CSV files\n",
    "units_with_data_files = []\n",
    "\n",
    "# Process and combine all raw CSV files\n",
    "dataframes = []\n",
    "for file in csv_files:\n",
    "    print(f\"\\nProcessing: {file}\")\n",
    "    \n",
    "    try:\n",
    "        # Load raw data\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        # Process the data (add EFA blocks, delivery dates, etc.)\n",
    "        df = process_raw_bess_data(df)\n",
    "        \n",
    "        # Add source identifier (extract unit name from filename)\n",
    "        unit_name = file.replace('.csv', '')\n",
    "        df['source_file'] = unit_name\n",
    "        \n",
    "        # Track which units actually have data files\n",
    "        if 'auctionUnit' in df.columns and len(df) > 0:\n",
    "            unit_codes = df['auctionUnit'].unique()\n",
    "            units_with_data_files.extend(unit_codes)\n",
    "        \n",
    "        dataframes.append(df)\n",
    "        print(f\"  ✓ Processed {len(df):,} records\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error processing {file}: {e}\")\n",
    "\n",
    "# Create a filtered mapping file that only includes units with actual data\n",
    "units_with_data_files = list(set(units_with_data_files))  # Remove duplicates\n",
    "filtered_mapping = mapping_df[mapping_df['Unit (NESO)'].isin(units_with_data_files)]\n",
    "\n",
    "print(f\"\\nFiltered mapping:\")\n",
    "print(f\"  Units with data files: {sorted(units_with_data_files)}\")\n",
    "print(f\"  Units excluded (no data files): {sorted(set(mapping_df['Unit (NESO)']) - set(units_with_data_files))}\")\n",
    "print(f\"  Filtered mapping has {len(filtered_mapping)} units (down from {len(mapping_df)})\")\n",
    "\n",
    "# Save the filtered mapping file\n",
    "filtered_mapping.to_csv('Battery name mapping filtered.csv', index=False)\n",
    "print(f\"  ✓ Saved filtered mapping: Battery name mapping filtered.csv\")\n",
    "\n",
    "# Combine all dataframes\n",
    "if dataframes:\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # Save the merged dataset - navigate to processed_data\n",
    "    os.chdir('../processed_data')\n",
    "    merged_df.to_csv('merged_17_units_updated.csv', index=False)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"✓ Merged dataset saved: merged_17_units_updated.csv\")\n",
    "    print(f\"✓ Total records: {len(merged_df):,}\")\n",
    "    print(f\"✓ Total units with data: {merged_df['auctionUnit'].nunique()}\")\n",
    "    print(f\"✓ Total companies: {merged_df['source_file'].str.split(' - ').str[0].nunique()}\")\n",
    "    print(f\"✓ Date range: {merged_df['deliveryStart'].min()} to {merged_df['deliveryStart'].max()}\")\n",
    "    print(f\"✓ Columns: {list(merged_df.columns)}\")\n",
    "    print(f\"✓ Units excluded from original mapping (no data files): SWDN-1, ALVB-1\")\n",
    "    \n",
    "    # Show sample of units\n",
    "    print(f\"\\nUnits included:\")\n",
    "    for unit in sorted(merged_df['source_file'].unique()):\n",
    "        count = len(merged_df[merged_df['source_file'] == unit])\n",
    "        # Get the auctionUnit code for this source file\n",
    "        unit_code = merged_df[merged_df['source_file'] == unit]['auctionUnit'].iloc[0]\n",
    "        print(f\"  - {unit} ({unit_code}): {count:,} records\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No files were successfully processed!\")\n",
    "\n",
    "# Return to original directory (analysis_notebooks)\n",
    "os.chdir(original_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0390bb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up 17 individual updated files...\n",
      "  ✓ Removed: GS - Breach Farm_updated.csv\n",
      "  ✓ Removed: EDF - Coventry_updated.csv\n",
      "  ✓ Removed: GS - Hulley Road_updated.csv\n",
      "  ✓ Removed: EDF - Tye Lane_updated.csv\n",
      "  ✓ Removed: Conrad - Blackpool_updated.csv\n",
      "  ✓ Removed: EDF - Kemsley_updated.csv\n",
      "  ✓ Removed: EDF - Bustleholm_updated.csv\n",
      "  ✓ Removed: Centrica - Roosecote_updated.csv\n",
      "  ✓ Removed: EDF - Cowley_updated.csv\n",
      "  ✓ Removed: Scottish - Whitelee_updated.csv\n",
      "  ✓ Removed: GS - Larport_updated.csv\n",
      "  ✓ Removed: Conrad - Midsomer_updated.csv\n",
      "  ✓ Removed: Conrad - Torquay_updated.csv\n",
      "  ✓ Removed: GS - Cenin_updated.csv\n",
      "  ✓ Removed: Conrad - Winchester_updated.csv\n",
      "  ✓ Removed: GS - Port of Tilbury_updated.csv\n",
      "  ✓ Removed: GS - Lascar_updated.csv\n",
      "Cleanup complete! Only merged_17_units_updated.csv remains in processed_data.\n"
     ]
    }
   ],
   "source": [
    "# Clean up any existing individual updated files\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Store current directory and navigate to processed_data\n",
    "original_dir = os.getcwd()\n",
    "os.chdir('../processed_data')\n",
    "\n",
    "updated_files = glob.glob('*_updated.csv')\n",
    "updated_files = [f for f in updated_files if f != 'merged_17_units_updated.csv']\n",
    "\n",
    "print(f\"Cleaning up {len(updated_files)} individual updated files...\")\n",
    "for file in updated_files:\n",
    "    try:\n",
    "        os.remove(file)\n",
    "        print(f\"  ✓ Removed: {file}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  - File not found: {file}\")\n",
    "\n",
    "# Return to original directory\n",
    "os.chdir(original_dir)\n",
    "print(\"Cleanup complete! Only merged_17_units_updated.csv remains in processed_data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
